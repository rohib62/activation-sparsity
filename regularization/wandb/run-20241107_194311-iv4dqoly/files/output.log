Loading model with tensor parallelism...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.32it/s]
Loading tokenizer...
Initializing trainer...
Loading dataset...
Setting up optimizer and scheduler...
Starting training...
Epoch 1:   0%|                                                                                                                | 0/13001 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/riyasatohib_cohere_com/repos/teal_clone/regularization/act_reg.py", line 324, in <module>
    main()
  File "/home/riyasatohib_cohere_com/repos/teal_clone/regularization/act_reg.py", line 308, in main
    model = train(
            ^^^^^^
  File "/home/riyasatohib_cohere_com/repos/teal_clone/regularization/act_reg.py", line 203, in train
    reg_loss = trainer.get_regularization_loss()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/riyasatohib_cohere_com/repos/teal_clone/regularization/act_reg.py", line 151, in get_regularization_loss
    return sum(hook.regularization_loss for _, hook in self.hooks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!
