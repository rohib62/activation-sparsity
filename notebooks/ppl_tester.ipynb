{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riyasatohib_cohere_com/.conda/envs/teal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/riyasatohib_cohere_com/repos/models/command-r-refresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:11<00:00,  1.22it/s]\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: [{'role': 'user', 'content': 'Give three tips for staying healthy.'}]\n",
      "Tokenized length: 13\n",
      "Loss: 2.444206714630127\n",
      "Debug example perplexity: 11.52\n",
      "Evaluating perplexity on 250 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_dataset(dataset_name, subset=None, split=\"train\", size=None):\n",
    "    \"\"\"Load and optionally subset a dataset.\"\"\"\n",
    "    dataset = load_dataset(dataset_name, subset, split=split)\n",
    "    if size is not None:\n",
    "        dataset = dataset.select(range(size))\n",
    "    return dataset\n",
    "\n",
    "def format_alpaca_as_chat(example):\n",
    "    \"\"\"Format Alpaca data as Command-R chat messages.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    full_instruction = f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": full_instruction}]\n",
    "    return messages\n",
    "\n",
    "def eval_cohere_ppl(model, tokenizer, dataset, device=\"cuda\", debug=False):\n",
    "    \"\"\"Evaluate perplexity of Cohere model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(dataset):\n",
    "            messages = format_alpaca_as_chat(example)\n",
    "            \n",
    "            # Format with chat template and get target\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            total_loss += loss.item() * input_ids.size(1)\n",
    "            total_tokens += input_ids.size(1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Example input: {messages}\")\n",
    "                print(f\"Tokenized length: {input_ids.size(1)}\")\n",
    "                print(f\"Loss: {loss.item()}\")\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = torch.exp(torch.tensor(avg_loss))\n",
    "    return ppl.item()\n",
    "\n",
    "# Example usage in notebook cells:\n",
    "\n",
    "# Cell 1: Load model and tokenizer\n",
    "model_name = \"/home/riyasatohib_cohere_com/repos/models/command-r-refresh\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Cell 2: Load dataset\n",
    "dataset_size = 250  # Adjust as needed\n",
    "dataset = get_dataset(\n",
    "    \"tatsu-lab/alpaca\",\n",
    "    subset=None,\n",
    "    split=\"train\",\n",
    "    size=dataset_size\n",
    ")\n",
    "\n",
    "# Cell 3: Test with one example (debug mode)\n",
    "debug_ppl = eval_cohere_ppl(model, tokenizer, dataset, debug=True)\n",
    "print(f\"Debug example perplexity: {debug_ppl:.2f}\")\n",
    "\n",
    "# Cell 4: Run full evaluation\n",
    "print(f\"Evaluating perplexity on {dataset_size} examples...\")\n",
    "ppl = eval_cohere_ppl(model, tokenizer, dataset, debug=False)\n",
    "print(f\"Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Function Definitions\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_dataset(dataset_name, subset=None, split=\"train\", size=None):\n",
    "    \"\"\"Load and optionally subset a dataset.\"\"\"\n",
    "    dataset = load_dataset(dataset_name, subset, split=split)\n",
    "    if size is not None:\n",
    "        dataset = dataset.select(range(size))\n",
    "    print(f\"Loaded dataset with {len(dataset)} examples\")\n",
    "    return dataset\n",
    "\n",
    "def format_alpaca_as_chat(example):\n",
    "    \"\"\"Format Alpaca data as Command-R chat messages.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    full_instruction = f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "    messages = [{\"role\": \"user\", \"content\": full_instruction}]\n",
    "    return messages\n",
    "\n",
    "def eval_cohere_ppl(model, tokenizer, dataset, num_examples=None, device=\"cuda\", debug=False):\n",
    "    \"\"\"\n",
    "    Evaluate perplexity of Cohere model on dataset with clean progress display.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    lengths = []\n",
    "    \n",
    "    # Select subset if num_examples is specified\n",
    "    if num_examples is not None:\n",
    "        num_examples = min(num_examples, len(dataset))\n",
    "        dataset = dataset.select(range(num_examples))\n",
    "    else:\n",
    "        num_examples = len(dataset)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(dataset, total=num_examples, desc=\"Computing perplexity\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for example in pbar:\n",
    "            messages = format_alpaca_as_chat(example)\n",
    "            \n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            lengths.append(input_ids.size(1))\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(1)\n",
    "            total_tokens += input_ids.size(1)\n",
    "            \n",
    "            # Update progress bar with current PPL\n",
    "            current_ppl = torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "            pbar.set_postfix({'PPL': f'{current_ppl:.2f}'}, refresh=True)\n",
    "            \n",
    "            if debug and len(lengths) == 1:\n",
    "                print(f\"\\nExample input: {messages}\")\n",
    "                print(f\"Tokenized length: {input_ids.size(1)}\")\n",
    "                print(f\"Loss: {loss.item()}\")\n",
    "                break\n",
    "    \n",
    "    # Calculate final perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEvaluation Summary:\")\n",
    "    print(f\"Processed {num_examples:,} examples, {total_tokens:,} tokens\")\n",
    "    print(f\"Average tokens per example: {total_tokens/num_examples:.1f}\")\n",
    "    print(f\"Max length: {max(lengths)}, Min length: {min(lengths)}, Avg length: {sum(lengths)/len(lengths):.1f}\")\n",
    "    print(f\"Final Perplexity: {ppl:.2f}\")\n",
    "    \n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/riyasatohib_cohere_com/repos/models/command-r-refresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:10<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 52002 examples\n",
      "Starting full evaluation on 52002 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity: 100%|██████████| 500/500 [00:16<00:00, 29.87it/s, PPL=11.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary:\n",
      "Processed 500 examples, 11,379 tokens\n",
      "Average tokens per example: 22.8\n",
      "Max length: 110, Min length: 11, Avg length: 22.8\n",
      "Final Perplexity: 11.54\n",
      "Final Perplexity: 11.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model and Tokenizer\n",
    "model_name = \"/home/riyasatohib_cohere_com/repos/models/command-r-refresh\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Cell 3: Load Dataset\n",
    "# Change size=None to process all examples, or set a specific number\n",
    "dataset_size = None  # Try with 1000 first, then set to None for full dataset\n",
    "dataset = get_dataset(\n",
    "    \"tatsu-lab/alpaca\",\n",
    "    subset=None,\n",
    "    split=\"train\",\n",
    "    size=dataset_size\n",
    ")\n",
    "\n",
    "# Cell 4: Optional Debug Run (run this cell to test with one example)\n",
    "# debug_dataset = dataset.select(range(1))\n",
    "# debug_ppl = eval_cohere_ppl(model, tokenizer, dataset, num_examples=500, device=\"cuda\", debug=False)\n",
    "# # debug_ppl = eval_cohere_ppl(model, tokenizer, debug_dataset, debug=True)\n",
    "# print(f\"Debug example perplexity: {debug_ppl:.2f}\")\n",
    "\n",
    "# Cell 5: Full Evaluation\n",
    "print(f\"Starting full evaluation on {len(dataset)} examples...\")\n",
    "ppl = eval_cohere_ppl(model, tokenizer, dataset, num_examples=500, device=\"cuda\", debug=False)\n",
    "print(f\"Final Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Modification of the TEAL ppl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "# sys.path.append('../')\n",
    "sys.path.append('/home/riyasatohib_cohere_com/repos/teal_clone/utils/utils.py')\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from utils.utils import get_tokenizer, get_sparse_model\n",
    "from utils.eval_ppl import eval_ppl\n",
    "\n",
    "from teal.model import (\n",
    "    LlamaSparseForCausalLM, \n",
    "    LlamaSparseConfig,\n",
    "    MistralSparseForCausalLM, \n",
    "    MistralSparseConfig,\n",
    "    CohereSparseForCausalLM,\n",
    "    CohereSparseConfig\n",
    ")\n",
    "\n",
    "from utils.data import get_dataset\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "AutoConfig.register(\"llama_sparse\", LlamaSparseConfig)\n",
    "AutoConfig.register(\"mistral_sparse\", MistralSparseConfig)\n",
    "AutoConfig.register(\"cohere_sparse\", CohereSparseConfig)\n",
    "\n",
    "AutoModelForCausalLM.register(LlamaSparseConfig, LlamaSparseForCausalLM)\n",
    "AutoModelForCausalLM.register(MistralSparseConfig, MistralSparseForCausalLM)\n",
    "AutoModelForCausalLM.register(CohereSparseConfig, CohereSparseForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Parse command line arguments for the script.\")\n",
    "# parser.add_argument('--model_name', type=str, default=\"meta-llama/Llama-2-7b-hf\",help='Name of the model to use')\n",
    "# parser.add_argument('--hist_path', type=str, default=\"meta-llama/Llama-2-7b-hf\",help='Name of the model to use')\n",
    "# parser.add_argument('--teal_path', type=str, required=True,help='Path to the teal input')\n",
    "# parser.add_argument('--save_path', type=str, default=\"./model\", required=True,help='Path to the teal input')\n",
    "# parser.add_argument('--greedy_flag', action='store_true', help='Flag for greedy')\n",
    "# parser.add_argument('--sparsity', type=float, default=0.5, help='Sparsity level')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/riyasatohib_cohere_com/repos/models/command-r-refresh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"/home/riyasatohib_cohere_com/repos/models/command-r-refresh\"\n",
    "        self.hist_path = \"/home/riyasatohib_cohere_com/repos/teal_clone/models/command-r-refresh/histograms\"\n",
    "        self.teal_path = '/home/riyasatohib_cohere_com/repos/teal_clone/models'  # Required argument\n",
    "        self.save_path = \"/home/riyasatohib_cohere_com/repos/models/command-refresh-sparse/\"  # Default value though marked as required\n",
    "        self.greedy_flag = False\n",
    "        self.sparsity = 0.3\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([f\"{key}={value}\" for key, value in vars(self).items()])\n",
    "\n",
    "args = Args()\n",
    "args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type cohere to instantiate a model of type cohere_sparse. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:10<00:00,  1.32it/s]\n",
      "/home/riyasatohib_cohere_com/repos/teal_clone/utils/utils.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  histogram = torch.load(f\"{self.file_path}/histograms.pt\")\n"
     ]
    }
   ],
   "source": [
    "## Dataset has to be loaded like this for Command-r-compatibility\n",
    "# tokenizer = get_tokenizer(args.model_name)\n",
    "sps_model = get_sparse_model(args.model_name, device=\"auto\", histogram_path=args.hist_path, apply_prefill=False) # Add this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/riyasatohib_cohere_com/repos/models/command-r-refresh'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = None  # Try with 1000 first, then set to None for full dataset\n",
    "dataset = get_dataset(\n",
    "    \"tatsu-lab/alpaca\",\n",
    "    subset=None,\n",
    "    split=\"train\",\n",
    "    size=dataset_size\n",
    ")\n",
    "args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluating dense PPL\")\n",
    "# print(\"=\"*40)\n",
    "# dense_ppl = eval_ppl(model, tokenizer, device=\"cuda\", dataset=dataset, debug=False)\n",
    "# print(f\"PPL: {dense_ppl}\")\n",
    "\n",
    "\n",
    "# print(\"Evaluating sparse PPL at sparsity level: \", args.sparsity)\n",
    "# print(\"=\"*40)\n",
    "# if args.greedy_flag:\n",
    "#     print(\"Evaluating greedy PPL\")\n",
    "#     greedy_path = os.path.join(args.teal_path, \"lookup\")\n",
    "#     model.load_greedy_sparsities(greedy_path, args.sparsity)\n",
    "# else:\n",
    "#     print(\"Evaluating uniform PPL\")\n",
    "#     model.set_uniform_sparsity(args.sparsity)\n",
    "\n",
    "# sparse_ppl = eval_ppl(model, tokenizer, device=\"cuda\", dataset=dataset, debug=False)\n",
    "# print(f\"PPL: {sparse_ppl}\")\n",
    "\n",
    "# print(\"=\"*40)\n",
    "\n",
    "### Saving model\n",
    "# print(f\"saving the model\")\n",
    "# model.save_pretrained(args.save_path)\n",
    "# tokenizer.save_pretrained(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# set sparsity\n",
    "args.sparsity = 0.00000001\n",
    "\n",
    "print(\"Evaluating sparse PPL at sparsity level: \", args.sparsity)\n",
    "print(\"=\"*40)\n",
    "if args.greedy_flag:\n",
    "    print(\"Evaluating greedy PPL\")\n",
    "    greedy_path = os.path.join(args.teal_path, \"lookup\")\n",
    "    sps_model.load_greedy_sparsities(greedy_path, args.sparsity)\n",
    "else:\n",
    "    print(\"Evaluating uniform PPL\")\n",
    "    sps_model.set_uniform_sparsity(args.sparsity)\n",
    "    \n",
    "ppl = eval_cohere_ppl(sps_model, tokenizer, dataset, num_examples=500, device=\"cuda\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity: 100%|██████████| 500/500 [00:16<00:00, 30.23it/s, PPL=11.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary:\n",
      "Processed 500 examples, 11,379 tokens\n",
      "Average tokens per example: 22.8\n",
      "Max length: 110, Min length: 11, Avg length: 22.8\n",
      "Final Perplexity: 11.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ppl = eval_cohere_ppl(model, tokenizer, dataset, num_examples=500, device=\"cuda\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.53911018371582\n"
     ]
    }
   ],
   "source": [
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 52002 examples\n"
     ]
    }
   ],
   "source": [
    "dataset_size = None  # Try with 1000 first, then set to None for full dataset\n",
    "dataset = get_dataset(\n",
    "    \"tatsu-lab/alpaca\",\n",
    "    subset=None,\n",
    "    split=\"train\",\n",
    "    size=dataset_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying sparse model configuration...\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "Casting input hidden states to torch.float16 (this should not be happening)\n",
      "\n",
      "No issues found in model configuration\n",
      "\n",
      "First layer statistics:\n",
      "MLP:\n",
      "  gate: threshold=0.0002, achieved_sparsity=0.0069\n",
      "  up: threshold=0.0002, achieved_sparsity=0.0069\n",
      "  down: threshold=-0.0000, achieved_sparsity=0.0000\n",
      "Attention:\n",
      "  q: threshold=0.0002, achieved_sparsity=0.0069\n",
      "  k: threshold=0.0002, achieved_sparsity=0.0069\n",
      "  v: threshold=0.0002, achieved_sparsity=0.0069\n",
      "  o: threshold=-0.0001, achieved_sparsity=0.0000\n"
     ]
    }
   ],
   "source": [
    "def verify_sparse_model_setup(sps_model, tokenizer, dataset, verbose=True):\n",
    "    \"\"\"Verify sparse model configuration and test with actual dataset samples\"\"\"\n",
    "    issues_found = []\n",
    "    \n",
    "    # Check layer configurations\n",
    "    for i, layer in enumerate(sps_model.model.layers):\n",
    "        # Check MLP sparsification\n",
    "        for name in ['gate', 'up', 'down']:\n",
    "            thresh = layer.mlp.sparse_fns[name].threshold\n",
    "            distr = layer.mlp.sparse_fns[name].distr\n",
    "            \n",
    "            if abs(thresh) > 10:  # Unusually large threshold\n",
    "                issues_found.append(f\"Layer {i} MLP {name} has large threshold: {thresh}\")\n",
    "            \n",
    "            if distr is None:\n",
    "                issues_found.append(f\"Layer {i} MLP {name} missing distribution\")\n",
    "            else:\n",
    "                centers = distr.bin_centers\n",
    "                counts = distr.counts\n",
    "                if torch.any(torch.isnan(centers)) or torch.any(torch.isnan(counts)):\n",
    "                    issues_found.append(f\"Layer {i} MLP {name} has NaN in distribution\")\n",
    "                if centers.numel() == 0 or counts.sum() == 0:\n",
    "                    issues_found.append(f\"Layer {i} MLP {name} has empty distribution\")\n",
    "        \n",
    "        # Check Attention sparsification\n",
    "        for name in ['q', 'k', 'v', 'o']:\n",
    "            thresh = layer.self_attn.sparse_fns[name].threshold\n",
    "            distr = layer.self_attn.sparse_fns[name].distr\n",
    "            \n",
    "            if abs(thresh) > 10:\n",
    "                issues_found.append(f\"Layer {i} Attention {name} has large threshold: {thresh}\")\n",
    "            \n",
    "            if distr is None:\n",
    "                issues_found.append(f\"Layer {i} Attention {name} missing distribution\")\n",
    "            else:\n",
    "                centers = distr.bin_centers\n",
    "                counts = distr.counts\n",
    "                if torch.any(torch.isnan(centers)) or torch.any(torch.isnan(counts)):\n",
    "                    issues_found.append(f\"Layer {i} Attention {name} has NaN in distribution\")\n",
    "                if centers.numel() == 0 or counts.sum() == 0:\n",
    "                    issues_found.append(f\"Layer {i} Attention {name} has empty distribution\")\n",
    "    \n",
    "    # Test forward pass with actual dataset sample\n",
    "    try:\n",
    "        # Get first example from dataset\n",
    "        sample_text = dataset[0] if isinstance(dataset[0], str) else dataset[0]['text']\n",
    "        input_ids = tokenizer(sample_text, return_tensors=\"pt\").input_ids.to(next(sps_model.parameters()).device)\n",
    "        \n",
    "        # Track activation sparsity during forward pass\n",
    "        activation_stats = {}\n",
    "        \n",
    "        def hook_fn(name):\n",
    "            def _hook(module, input, output):\n",
    "                if hasattr(module, 'threshold'):\n",
    "                    mask = input[0].abs() > module.threshold\n",
    "                    sparsity = 1 - mask.float().mean().item()\n",
    "                    activation_stats[name] = sparsity\n",
    "            return _hook\n",
    "        \n",
    "        # Register hooks for first layer\n",
    "        first_layer = sps_model.model.layers[0]\n",
    "        hooks = []\n",
    "        for name in ['gate', 'up', 'down']:\n",
    "            hooks.append(first_layer.mlp.sparse_fns[name].register_forward_hook(hook_fn(f'mlp_{name}')))\n",
    "        for name in ['q', 'k', 'v', 'o']:\n",
    "            hooks.append(first_layer.self_attn.sparse_fns[name].register_forward_hook(hook_fn(f'attn_{name}')))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = sps_model(input_ids)\n",
    "            \n",
    "            if torch.any(torch.isnan(outputs.logits)):\n",
    "                issues_found.append(\"Forward pass produces NaN logits\")\n",
    "            if torch.any(torch.isinf(outputs.logits)):\n",
    "                issues_found.append(\"Forward pass produces Inf logits\")\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "            \n",
    "    except Exception as e:\n",
    "        issues_found.append(f\"Forward pass failed: {str(e)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        if issues_found:\n",
    "            print(\"\\nIssues found:\")\n",
    "            for issue in issues_found:\n",
    "                print(f\"- {issue}\")\n",
    "        else:\n",
    "            print(\"\\nNo issues found in model configuration\")\n",
    "            \n",
    "        # Print sample of thresholds and achieved sparsity\n",
    "        print(\"\\nFirst layer statistics:\")\n",
    "        first_layer = sps_model.model.layers[0]\n",
    "        print(\"MLP:\")\n",
    "        for name in ['gate', 'up', 'down']:\n",
    "            thresh = first_layer.mlp.sparse_fns[name].threshold\n",
    "            achieved_sparsity = activation_stats.get(f'mlp_{name}')\n",
    "            sparsity_str = f\"{achieved_sparsity:.4f}\" if achieved_sparsity is not None else \"N/A\"\n",
    "            print(f\"  {name}: threshold={thresh:.4f}, achieved_sparsity={sparsity_str}\")\n",
    "        \n",
    "        print(\"Attention:\")\n",
    "        for name in ['q', 'k', 'v', 'o']:\n",
    "            thresh = first_layer.self_attn.sparse_fns[name].threshold\n",
    "            achieved_sparsity = activation_stats.get(f'attn_{name}')\n",
    "            sparsity_str = f\"{achieved_sparsity:.4f}\" if achieved_sparsity is not None else \"N/A\"\n",
    "            print(f\"  {name}: threshold={thresh:.4f}, achieved_sparsity={sparsity_str}\")\n",
    "    \n",
    "    return issues_found, activation_stats\n",
    "\n",
    "# Usage:\n",
    "print(\"Verifying sparse model configuration...\")\n",
    "issues, act_stats = verify_sparse_model_setup(sps_model, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CohereSparseForCausalLM(\n",
       "  (model): CohereModel(\n",
       "    (embed_tokens): Embedding(256000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x CohereDecoderLayer(\n",
       "        (self_attn): CohereFlashAttention2(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): CohereRotaryEmbedding()\n",
       "          (sparse_fns): ModuleDict(\n",
       "            (q): SparsifyFn()\n",
       "            (k): SparsifyFn()\n",
       "            (v): SparsifyFn()\n",
       "            (o): SparsifyFn()\n",
       "          )\n",
       "        )\n",
       "        (mlp): CohereMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "          (sparse_fns): ModuleDict(\n",
       "            (gate): SparsifyFn()\n",
       "            (up): SparsifyFn()\n",
       "            (down): SparsifyFn()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): CohereLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): CohereLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sps_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/riyasatohib_cohere_com/repos/models/command-r-ref-sps/tokenizer_config.json',\n",
       " '/home/riyasatohib_cohere_com/repos/models/command-r-ref-sps/special_tokens_map.json',\n",
       " '/home/riyasatohib_cohere_com/repos/models/command-r-ref-sps/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Saving model\n",
    "print(f\"saving the model\")\n",
    "save_path='/home/riyasatohib_cohere_com/repos/models/command-r-ref-sps'\n",
    "sps_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
